{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## init"
      ],
      "metadata": {
        "id": "JWxbRKGh3RzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qqq transformers torchaudio datasets"
      ],
      "metadata": {
        "id": "RCAHK5kxBSHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os"
      ],
      "metadata": {
        "id": "u3xkTBGt2cwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_MOUNT_PATH = \"/content/drive\"\n",
        "DATA_PATH = f\"{DRIVE_MOUNT_PATH}/MyDrive/Shared/data\""
      ],
      "metadata": {
        "id": "1IyiqteO2ejj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(DRIVE_MOUNT_PATH)"
      ],
      "metadata": {
        "id": "jLWXtxDA2heR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Use wav2vec2 for speech emotion recognition on IEMOCAP dataset**\n",
        "---\n",
        "- ðŸš€ **objective**: run wav2vec2 as a feature extractor on IEMOCAP dataset, requires the data preprocessing of IEMOCAP dataset  \n",
        "- ðŸ§¯ **models**: wav2vec2\n",
        "- ðŸ“š **dataset**: IEMOCAP\n",
        "\n",
        "\n",
        "Resources\n",
        "- inspired by https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb#scrollTo=Fv62ShDsH5DZ"
      ],
      "metadata": {
        "id": "ebaFEkvH3R6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ configuration"
      ],
      "metadata": {
        "id": "VTM5mAoeArCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "import torch"
      ],
      "metadata": {
        "id": "E5slPz53BPEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"facebook/wav2vec2-large-960h\"\n",
        "feature_to_idx = {key: i for i, key in enumerate([\"wav\", \"sampling_rate\", \"filename\", \"label\", \"speaker\"])}\n",
        "label_list = [\"neu\", \"hap\", \"ang\", \"sad\", \"exc\", \"fru\"]\n",
        "num_labels = len(label_list)\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "pooling_mode = \"max\"\n",
        "test_split_size = 0.2\n",
        "target_sampling_rate = 16000\n",
        "\n",
        "DEBUG_SIZE = 10"
      ],
      "metadata": {
        "id": "c9FAPgXIKa04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/wav2vec2-iemocap-speech-emotion-recognition\",\n",
        "    label_names=label_list,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=1.0,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    save_steps=10,\n",
        "    eval_steps=10,\n",
        "    logging_steps=10,\n",
        "    learning_rate=1e-4,\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "OeMVreG3-6qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“š data\n",
        "- ~torchaudio implemented a `dataset` to load IEMOCAP. Later in the script, we train the model with a `Trainer` from hugginface, therefore we prefer translating the pytorch dataset into a `transformers.Dataset` for convenience and compatibility.~\n",
        "- the Trainer class expects an argument `train_dataset` to be of type torch.utils.data.Dataset (see [documentation](https://huggingface.co/docs/transformers/main_classes/trainer)) --> we use a torch dataset instead of a Hugginface dataset"
      ],
      "metadata": {
        "id": "w9lfgKcGHlQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/audio/master/generated/torchaudio.datasets.IEMOCAP.html\n",
        "from torchaudio.datasets import IEMOCAP\n",
        "\n",
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import random_split, Dataset, DataLoader, SubsetRandomSampler"
      ],
      "metadata": {
        "id": "63mPgt-MR8p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n",
        "target_sampling_rate = processor.feature_extractor.sampling_rate"
      ],
      "metadata": {
        "id": "3h-qBGonTgmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomIEMOCAP(Dataset):\n",
        "  def __init__(self, data, processor):\n",
        "    self.data = data\n",
        "    self.processor = processor\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    wav, _, _, label, _ = self.data[index]\n",
        "    inputs = self.processor(wav.squeeze(), sampling_rate=target_sampling_rate)\n",
        "    inputs[\"labels\"] = label2id[label]\n",
        "\n",
        "    return inputs\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "lEkXSO7-wI2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iemocap = IEMOCAP(root=DATA_PATH) # in function, path = root / \"IEMOCAP\"\n",
        "dataset = CustomIEMOCAP(data=iemocap, processor=processor)\n",
        "train_ds, test_ds = random_split(dataset, [test_split_size, 1-test_split_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "dataset[0]"
      ],
      "metadata": {
        "id": "TkLW4D2ZwYCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # generate a huggingface dataset\n",
        "# iemocap = IEMOCAP(root=DATA_PATH) # in function, path = root / \"IEMOCAP\"\n",
        "\n",
        "# def _generate_dicts(ds: IEMOCAP):\n",
        "#   for i in range(min(DEBUG_SIZE, len(ds))):\n",
        "#     wav_path, sample_rate, filename, label, speaker = ds.get_metadata(i)\n",
        "#     yield {\n",
        "#         \"audio\": DATA_PATH + '/IEMOCAP/' + wav_path,\n",
        "#         \"label\": label, \n",
        "#         \"speaker\": speaker\n",
        "#     }\n",
        "    \n",
        "\n",
        "# dataset = Dataset.from_generator(lambda: _generate_dicts(iemocap)).cast_column(\"audio\", Audio(sampling_rate=target_sampling_rate))\n",
        "# label_list = dataset.unique('label')\n",
        "# num_labels = len(label_list)\n",
        "\n",
        "# split_dataset = dataset.train_test_split(test_size=test_split_size)\n",
        "# train_ds, test_ds = split_dataset['train'], split_dataset['test']\n",
        "\n",
        "# print(dataset)\n",
        "# print(train_ds)\n",
        "# print(test_ds)"
      ],
      "metadata": {
        "id": "Mf4KEhlzIJ2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸšœ model definition"
      ],
      "metadata": {
        "id": "_DZWd3CGHnnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Model, Wav2Vec2PreTrainedModel\n",
        "from transformers import AutoConfig\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss"
      ],
      "metadata": {
        "id": "OAJrSE-vR4OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "config = AutoConfig.from_pretrained(\n",
        "  model_name_or_path,\n",
        "  num_labels=num_labels,\n",
        "  label2id=label2id,\n",
        "  id2label=id2label,\n",
        "  finetuning_task=\"wav2vec2_clf\",\n",
        ")\n",
        "setattr(config, 'pooling_mode', pooling_mode)\n",
        "config"
      ],
      "metadata": {
        "id": "a-JbmAoyT6v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Union, Tuple\n",
        "from transformers.file_utils import ModelOutput\n",
        "\n",
        "@dataclass\n",
        "class SpeechClassifierOutput(ModelOutput):\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "metadata": {
        "id": "54M_I54lQ-QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Wav2Vec2ClassificationHead(nn.Module):\n",
        "  \"\"\"Head for wav2vec classification task.\"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.dropout = nn.Dropout(config.final_dropout)\n",
        "    self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "  def forward(self, features, **kwargs):\n",
        "    x = features\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense(x)\n",
        "    x = torch.tanh(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.num_labels = config.num_labels\n",
        "    self.pooling_mode = config.pooling_mode\n",
        "    self.config = config\n",
        "\n",
        "    self.wav2vec2 = Wav2Vec2Model(config)\n",
        "    self.classifier = Wav2Vec2ClassificationHead(config)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def freeze_feature_extractor(self):\n",
        "    self.wav2vec2.feature_extractor._freeze_parameters()\n",
        "\n",
        "  def merged_strategy(\n",
        "      \n",
        "      self,\n",
        "      hidden_states,\n",
        "      mode=\"mean\"\n",
        "  ):\n",
        "    if mode == \"mean\":\n",
        "        outputs = torch.mean(hidden_states, dim=1)\n",
        "    elif mode == \"sum\":\n",
        "        outputs = torch.sum(hidden_states, dim=1)\n",
        "    elif mode == \"max\":\n",
        "        outputs = torch.max(hidden_states, dim=1)[0]\n",
        "    else:\n",
        "        raise Exception(\n",
        "            \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      input_values,\n",
        "      attention_mask=None,\n",
        "      output_attentions=None,\n",
        "      output_hidden_states=None,\n",
        "      return_dict=None,\n",
        "      labels=None,\n",
        "  ):\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "    outputs = self.wav2vec2(\n",
        "        input_values,\n",
        "        attention_mask=attention_mask,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "    )\n",
        "    hidden_states = outputs[0]\n",
        "    hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
        "    logits = self.classifier(hidden_states)\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "    if not return_dict:\n",
        "      output = (logits,) + outputs[2:]\n",
        "      return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "    return SpeechClassifierOutput(\n",
        "      loss=loss,\n",
        "      logits=logits,\n",
        "      hidden_states=outputs.hidden_states,\n",
        "      attentions=outputs.attentions,\n",
        "    )"
      ],
      "metadata": {
        "id": "P1HzzCqPAuSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    config=config,\n",
        ")"
      ],
      "metadata": {
        "id": "C7aPeO-fGBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.freeze_feature_extractor()"
      ],
      "metadata": {
        "id": "qlYFwx5XTCjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸƒâ€â™€ï¸ training routine"
      ],
      "metadata": {
        "id": "hpjqc0qlH5u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, Union, Tuple, Optional\n",
        "from packaging import version\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from transformers import Trainer, is_apex_available, Wav2Vec2Processor, EvalPrediction, ProcessorMixin\n",
        "\n",
        "if is_apex_available():\n",
        "    from apex import amp \n",
        "    # Apex is a PyTorch add-on package from NVIDIA with capabilities for automatic mixed precision (AMP) and distributed training.\n",
        "    # https://www.ibm.com/docs/en/wmlce/1.6.1?topic=frameworks-getting-started-apex\n",
        "\n",
        "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
        "  _is_native_amp_available = True\n",
        "  from torch.cuda.amp import autocast"
      ],
      "metadata": {
        "id": "3frD5uDaR872"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "  processor: Wav2Vec2Processor\n",
        "  padding: Union[bool, str] = True\n",
        "  max_length: Optional[int] = None\n",
        "  max_length_labels: Optional[int] = None\n",
        "  pad_to_multiple_of: Optional[int] = None\n",
        "  pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "  def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "    input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n",
        "    label_features = [feature[\"labels\"] for feature in features]\n",
        "\n",
        "    d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
        "\n",
        "    batch = self.processor.pad(\n",
        "      input_features,\n",
        "      padding=self.padding,\n",
        "      max_length=self.max_length,\n",
        "      pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "      return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
        "    # print('batch', batch)\n",
        "    return batch"
      ],
      "metadata": {
        "id": "goTHilxaR3R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
      ],
      "metadata": {
        "id": "mqZbCzVJSmMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CTCTrainer(Trainer):\n",
        "  def training_step(self, processor: ProcessorMixin, inputs) -> torch.Tensor:\n",
        "    self.model.train()\n",
        "    inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "    loss = self.compute_loss(self.model, inputs)\n",
        "\n",
        "    if self.args.gradient_accumulation_steps > 1:\n",
        "      loss = loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "    self.scaler.scale(loss).backward()\n",
        "\n",
        "    return loss.detach()"
      ],
      "metadata": {
        "id": "5NxWSqMKGkRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "metadata": {
        "id": "eH9EZ7o3SWip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "trainer = CTCTrainer(\n",
        "  model=model,\n",
        "  data_collator=data_collator,\n",
        "  args=training_args,\n",
        "  compute_metrics=compute_metrics,\n",
        "  train_dataset=train_ds,\n",
        "  eval_dataset=test_ds,\n",
        "  tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "d1lLo-_vG8ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§ª experiments"
      ],
      "metadata": {
        "id": "QKJKwY4_IFCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WANDB_DISABLED = \"false\"\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "D9piW_jCHBav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}