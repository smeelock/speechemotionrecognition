{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd5b8c5",
   "metadata": {
    "id": "JWxbRKGh3RzQ",
    "papermill": {
     "duration": 0.008344,
     "end_time": "2023-03-06T19:40:59.928813",
     "exception": false,
     "start_time": "2023-03-06T19:40:59.920469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595a0d30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:40:59.945340Z",
     "iopub.status.busy": "2023-03-06T19:40:59.944141Z",
     "iopub.status.idle": "2023-03-06T19:41:11.670909Z",
     "shell.execute_reply": "2023-03-06T19:41:11.669673Z"
    },
    "id": "RCAHK5kxBSHF",
    "outputId": "e288ac84-2c76-4270-df9e-2d8a3d46433c",
    "papermill": {
     "duration": 11.738832,
     "end_time": "2023-03-06T19:41:11.674623",
     "exception": false,
     "start_time": "2023-03-06T19:40:59.935791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqq transformers torchaudio datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4707e642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:11.690426Z",
     "iopub.status.busy": "2023-03-06T19:41:11.690078Z",
     "iopub.status.idle": "2023-03-06T19:41:12.762907Z",
     "shell.execute_reply": "2023-03-06T19:41:12.761358Z"
    },
    "id": "1IyiqteO2ejj",
    "papermill": {
     "duration": 1.08444,
     "end_time": "2023-03-06T19:41:12.766199",
     "exception": false,
     "start_time": "2023-03-06T19:41:11.681759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# env variables\n",
    "# the datapath is actually \"/kaggle/input/iemocapfullrelease\" but we use a symlink to get to the dataset\n",
    "DATA_PATH = \"/kaggle/working\"\n",
    "\n",
    "# we need a symling because torchaudio.datasets.IEMOCAP adds a \"/IEMOCAP\" to the data path, but the dataset is at /kaggle/input/iemocapfullrelease/IEMOCAP_full_release and the directory is read-only\n",
    "!ln -s /kaggle/input/iemocapfullrelease/IEMOCAP_full_release /kaggle/working/IEMOCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4710b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:12.782440Z",
     "iopub.status.busy": "2023-03-06T19:41:12.782052Z",
     "iopub.status.idle": "2023-03-06T19:41:15.757270Z",
     "shell.execute_reply": "2023-03-06T19:41:15.756012Z"
    },
    "papermill": {
     "duration": 2.986371,
     "end_time": "2023-03-06T19:41:15.759858",
     "exception": false,
     "start_time": "2023-03-06T19:41:12.773487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"wandb-api-token\") \n",
    "wandb.login(key=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e45b7d",
   "metadata": {
    "id": "ebaFEkvH3R6W",
    "papermill": {
     "duration": 0.007141,
     "end_time": "2023-03-06T19:41:15.774327",
     "exception": false,
     "start_time": "2023-03-06T19:41:15.767186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Use wav2vec2 for speech emotion recognition on IEMOCAP dataset**\n",
    "---\n",
    "- ðŸš€ **objective**: run wav2vec2 as a feature extractor on IEMOCAP dataset, requires the data preprocessing of IEMOCAP dataset  \n",
    "- ðŸ§¯ **models**: wav2vec2\n",
    "- ðŸ“š **dataset**: IEMOCAP\n",
    "\n",
    "\n",
    "Resources\n",
    "- inspired by https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb#scrollTo=Fv62ShDsH5DZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e057b6",
   "metadata": {
    "id": "VTM5mAoeArCU",
    "papermill": {
     "duration": 0.006975,
     "end_time": "2023-03-06T19:41:15.788488",
     "exception": false,
     "start_time": "2023-03-06T19:41:15.781513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## âš™ï¸ configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb27d8a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:15.804383Z",
     "iopub.status.busy": "2023-03-06T19:41:15.804022Z",
     "iopub.status.idle": "2023-03-06T19:41:26.061369Z",
     "shell.execute_reply": "2023-03-06T19:41:26.059939Z"
    },
    "id": "E5slPz53BPEB",
    "papermill": {
     "duration": 10.2688,
     "end_time": "2023-03-06T19:41:26.064388",
     "exception": false,
     "start_time": "2023-03-06T19:41:15.795588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d2cfd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:26.082074Z",
     "iopub.status.busy": "2023-03-06T19:41:26.080461Z",
     "iopub.status.idle": "2023-03-06T19:41:26.093147Z",
     "shell.execute_reply": "2023-03-06T19:41:26.092072Z"
    },
    "id": "c9FAPgXIKa04",
    "papermill": {
     "duration": 0.023789,
     "end_time": "2023-03-06T19:41:26.095549",
     "exception": false,
     "start_time": "2023-03-06T19:41:26.071760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"facebook/wav2vec2-base-960h\"\n",
    "feature_to_idx = {key: i for i, key in enumerate([\"wav\", \"sampling_rate\", \"filename\", \"label\", \"speaker\"])}\n",
    "label_list = [\"neu\", \"hap\", \"ang\", \"sad\", \"exc\", \"fru\"]\n",
    "num_labels = len(label_list)\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "pooling_mode = \"max\"\n",
    "test_split_size = 0.2\n",
    "target_sampling_rate = 16000\n",
    "\n",
    "DEBUG_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2b3671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:26.112809Z",
     "iopub.status.busy": "2023-03-06T19:41:26.111155Z",
     "iopub.status.idle": "2023-03-06T19:41:26.203895Z",
     "shell.execute_reply": "2023-03-06T19:41:26.202628Z"
    },
    "id": "OeMVreG3-6qs",
    "papermill": {
     "duration": 0.103421,
     "end_time": "2023-03-06T19:41:26.206340",
     "exception": false,
     "start_time": "2023-03-06T19:41:26.102919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/wav2vec2-iemocap-speech-emotion-recognition\",\n",
    "    label_names=label_list,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1.0,\n",
    "    fp16=torch.cuda.is_available(), # whether to use fp16 16-bit (mixed) precision training instead of 32-bit training\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"wandb\",\n",
    "    half_precision_backend=\"auto\", # shoud be 'cuda_amp' half precision backend in Colab\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadde0a",
   "metadata": {
    "id": "w9lfgKcGHlQF",
    "papermill": {
     "duration": 0.007024,
     "end_time": "2023-03-06T19:41:26.220777",
     "exception": false,
     "start_time": "2023-03-06T19:41:26.213753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸ“š data\n",
    "- ~torchaudio implemented a `dataset` to load IEMOCAP. Later in the script, we train the model with a `Trainer` from hugginface, therefore we prefer translating the pytorch dataset into a `transformers.Dataset` for convenience and compatibility.~\n",
    "- the Trainer class expects an argument `train_dataset` to be of type torch.utils.data.Dataset (see [documentation](https://huggingface.co/docs/transformers/main_classes/trainer)) --> we use a torch dataset instead of a Hugginface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820a3f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:26.237504Z",
     "iopub.status.busy": "2023-03-06T19:41:26.236469Z",
     "iopub.status.idle": "2023-03-06T19:41:26.658858Z",
     "shell.execute_reply": "2023-03-06T19:41:26.657567Z"
    },
    "id": "63mPgt-MR8p0",
    "papermill": {
     "duration": 0.433929,
     "end_time": "2023-03-06T19:41:26.661824",
     "exception": false,
     "start_time": "2023-03-06T19:41:26.227895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/audio/master/generated/torchaudio.datasets.IEMOCAP.html\n",
    "from torchaudio.datasets import IEMOCAP\n",
    "\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c902a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:26.679519Z",
     "iopub.status.busy": "2023-03-06T19:41:26.678513Z",
     "iopub.status.idle": "2023-03-06T19:41:27.928631Z",
     "shell.execute_reply": "2023-03-06T19:41:27.927356Z"
    },
    "id": "3h-qBGonTgmf",
    "outputId": "248f4e86-feb2-448a-b029-98ee6813ca56",
    "papermill": {
     "duration": 1.263325,
     "end_time": "2023-03-06T19:41:27.933259",
     "exception": false,
     "start_time": "2023-03-06T19:41:26.669934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6306c77e8fb340a080c790b168ed3493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012a855fe4c04e4da81e5ce9f291c802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84967642b97b4d71b081131b81d27538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81741c4a91864f7f8cd5c6578f8702cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0437f50134154151b3cdd436fa4f9f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "708d9d88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:27.951617Z",
     "iopub.status.busy": "2023-03-06T19:41:27.950938Z",
     "iopub.status.idle": "2023-03-06T19:41:27.958445Z",
     "shell.execute_reply": "2023-03-06T19:41:27.957471Z"
    },
    "id": "lEkXSO7-wI2t",
    "papermill": {
     "duration": 0.018971,
     "end_time": "2023-03-06T19:41:27.960857",
     "exception": false,
     "start_time": "2023-03-06T19:41:27.941886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomIEMOCAP(Dataset):\n",
    "  def __init__(self, data, processor):\n",
    "    self.data = data\n",
    "    self.processor = processor\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    wav, _, _, label, _ = self.data[index]\n",
    "    inputs = self.processor(wav.squeeze(), sampling_rate=target_sampling_rate)\n",
    "    inputs[\"labels\"] = label2id[label]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14433020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:27.978186Z",
     "iopub.status.busy": "2023-03-06T19:41:27.977296Z",
     "iopub.status.idle": "2023-03-06T19:41:31.707291Z",
     "shell.execute_reply": "2023-03-06T19:41:31.705976Z"
    },
    "id": "TkLW4D2ZwYCl",
    "outputId": "bb7e8a25-ea39-4ede-9acc-dd5dc6c9aca0",
    "papermill": {
     "duration": 3.741778,
     "end_time": "2023-03-06T19:41:31.710241",
     "exception": false,
     "start_time": "2023-03-06T19:41:27.968463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': [array([-0.38823238, -0.3835091 , -0.2937664 , ..., -0.20402369,\n",
       "       -0.24417175, -0.32210618], dtype=float32)], 'labels': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemocap = IEMOCAP(root=DATA_PATH) # in function, path = root / \"IEMOCAP\"\n",
    "dataset = CustomIEMOCAP(data=iemocap, processor=processor)\n",
    "train_ds, test_ds = random_split(dataset, [test_split_size, 1-test_split_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff326d",
   "metadata": {
    "id": "_DZWd3CGHnnc",
    "papermill": {
     "duration": 0.008479,
     "end_time": "2023-03-06T19:41:31.729126",
     "exception": false,
     "start_time": "2023-03-06T19:41:31.720647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸšœ model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70698f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:31.747097Z",
     "iopub.status.busy": "2023-03-06T19:41:31.746120Z",
     "iopub.status.idle": "2023-03-06T19:41:31.861731Z",
     "shell.execute_reply": "2023-03-06T19:41:31.860609Z"
    },
    "id": "OAJrSE-vR4OO",
    "papermill": {
     "duration": 0.127295,
     "end_time": "2023-03-06T19:41:31.864457",
     "exception": false,
     "start_time": "2023-03-06T19:41:31.737162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model, Wav2Vec2PreTrainedModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15a7b9b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:31.882748Z",
     "iopub.status.busy": "2023-03-06T19:41:31.882380Z",
     "iopub.status.idle": "2023-03-06T19:41:31.993420Z",
     "shell.execute_reply": "2023-03-06T19:41:31.991929Z"
    },
    "id": "a-JbmAoyT6v0",
    "outputId": "e43096d9-efef-4d2a-ad9e-f7c752678917",
    "papermill": {
     "duration": 0.124444,
     "end_time": "2023-03-06T19:41:31.997318",
     "exception": false,
     "start_time": "2023-03-06T19:41:31.872874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Config {\n",
       "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"adapter_kernel_size\": 3,\n",
       "  \"adapter_stride\": 2,\n",
       "  \"add_adapter\": false,\n",
       "  \"apply_spec_augment\": true,\n",
       "  \"architectures\": [\n",
       "    \"Wav2Vec2ForCTC\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classifier_proj_size\": 256,\n",
       "  \"codevector_dim\": 256,\n",
       "  \"contrastive_logits_temperature\": 0.1,\n",
       "  \"conv_bias\": false,\n",
       "  \"conv_dim\": [\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512\n",
       "  ],\n",
       "  \"conv_kernel\": [\n",
       "    10,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    2,\n",
       "    2\n",
       "  ],\n",
       "  \"conv_stride\": [\n",
       "    5,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2\n",
       "  ],\n",
       "  \"ctc_loss_reduction\": \"sum\",\n",
       "  \"ctc_zero_infinity\": false,\n",
       "  \"diversity_loss_weight\": 0.1,\n",
       "  \"do_stable_layer_norm\": false,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"feat_extract_activation\": \"gelu\",\n",
       "  \"feat_extract_dropout\": 0.0,\n",
       "  \"feat_extract_norm\": \"group\",\n",
       "  \"feat_proj_dropout\": 0.1,\n",
       "  \"feat_quantizer_dropout\": 0.0,\n",
       "  \"final_dropout\": 0.1,\n",
       "  \"finetuning_task\": \"wav2vec2_clf\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout\": 0.1,\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"neu\",\n",
       "    \"1\": \"hap\",\n",
       "    \"2\": \"ang\",\n",
       "    \"3\": \"sad\",\n",
       "    \"4\": \"exc\",\n",
       "    \"5\": \"fru\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"ang\": 2,\n",
       "    \"exc\": 4,\n",
       "    \"fru\": 5,\n",
       "    \"hap\": 1,\n",
       "    \"neu\": 0,\n",
       "    \"sad\": 3\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"layerdrop\": 0.1,\n",
       "  \"mask_feature_length\": 10,\n",
       "  \"mask_feature_min_masks\": 0,\n",
       "  \"mask_feature_prob\": 0.0,\n",
       "  \"mask_time_length\": 10,\n",
       "  \"mask_time_min_masks\": 2,\n",
       "  \"mask_time_prob\": 0.05,\n",
       "  \"model_type\": \"wav2vec2\",\n",
       "  \"num_adapter_layers\": 3,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_codevector_groups\": 2,\n",
       "  \"num_codevectors_per_group\": 320,\n",
       "  \"num_conv_pos_embedding_groups\": 16,\n",
       "  \"num_conv_pos_embeddings\": 128,\n",
       "  \"num_feat_extract_layers\": 7,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_negatives\": 100,\n",
       "  \"output_hidden_size\": 768,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooling_mode\": \"max\",\n",
       "  \"proj_codevector_dim\": 256,\n",
       "  \"tdnn_dilation\": [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    1,\n",
       "    1\n",
       "  ],\n",
       "  \"tdnn_dim\": [\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    1500\n",
       "  ],\n",
       "  \"tdnn_kernel\": [\n",
       "    5,\n",
       "    3,\n",
       "    3,\n",
       "    1,\n",
       "    1\n",
       "  ],\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"use_weighted_layer_sum\": false,\n",
       "  \"vocab_size\": 32,\n",
       "  \"xvector_output_dim\": 512\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model configuration\n",
    "config = AutoConfig.from_pretrained(\n",
    "  model_name_or_path,\n",
    "  num_labels=num_labels,\n",
    "  label2id=label2id,\n",
    "  id2label=id2label,\n",
    "  finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e758d916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:32.015396Z",
     "iopub.status.busy": "2023-03-06T19:41:32.015043Z",
     "iopub.status.idle": "2023-03-06T19:41:32.022684Z",
     "shell.execute_reply": "2023-03-06T19:41:32.021644Z"
    },
    "id": "54M_I54lQ-QJ",
    "papermill": {
     "duration": 0.019595,
     "end_time": "2023-03-06T19:41:32.025336",
     "exception": false,
     "start_time": "2023-03-06T19:41:32.005741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d05fc903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:32.043285Z",
     "iopub.status.busy": "2023-03-06T19:41:32.042978Z",
     "iopub.status.idle": "2023-03-06T19:41:32.060403Z",
     "shell.execute_reply": "2023-03-06T19:41:32.059400Z"
    },
    "id": "P1HzzCqPAuSH",
    "papermill": {
     "duration": 0.029398,
     "end_time": "2023-03-06T19:41:32.062860",
     "exception": false,
     "start_time": "2023-03-06T19:41:32.033462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "  \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.dropout = nn.Dropout(config.final_dropout)\n",
    "    self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "  def forward(self, features, **kwargs):\n",
    "    x = features\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.out_proj(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "    self.num_labels = config.num_labels\n",
    "    self.pooling_mode = config.pooling_mode\n",
    "    self.config = config\n",
    "\n",
    "    self.wav2vec2 = Wav2Vec2Model(config)\n",
    "    self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def freeze_feature_extractor(self):\n",
    "    self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "  def merged_strategy(\n",
    "      \n",
    "      self,\n",
    "      hidden_states,\n",
    "      mode=\"mean\"\n",
    "  ):\n",
    "    if mode == \"mean\":\n",
    "        outputs = torch.mean(hidden_states, dim=1)\n",
    "    elif mode == \"sum\":\n",
    "        outputs = torch.sum(hidden_states, dim=1)\n",
    "    elif mode == \"max\":\n",
    "        outputs = torch.max(hidden_states, dim=1)[0]\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "    return outputs\n",
    "\n",
    "  def forward(\n",
    "      self,\n",
    "      input_values,\n",
    "      attention_mask=None,\n",
    "      output_attentions=None,\n",
    "      output_hidden_states=None,\n",
    "      return_dict=None,\n",
    "      labels=None,\n",
    "  ):\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "    outputs = self.wav2vec2(\n",
    "        input_values,\n",
    "        attention_mask=attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    hidden_states = outputs[0]\n",
    "    hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "    logits = self.classifier(hidden_states)\n",
    "\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      loss_fct = CrossEntropyLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "    if not return_dict:\n",
    "      output = (logits,) + outputs[2:]\n",
    "      return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    return SpeechClassifierOutput(\n",
    "      loss=loss,\n",
    "      logits=logits,\n",
    "      hidden_states=outputs.hidden_states,\n",
    "      attentions=outputs.attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df76b5eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:32.081169Z",
     "iopub.status.busy": "2023-03-06T19:41:32.080859Z",
     "iopub.status.idle": "2023-03-06T19:41:39.434196Z",
     "shell.execute_reply": "2023-03-06T19:41:39.433062Z"
    },
    "id": "C7aPeO-fGBt-",
    "outputId": "b40f4286-bf78-470e-cb88-ff06e3cfc713",
    "papermill": {
     "duration": 7.365847,
     "end_time": "2023-03-06T19:41:39.437084",
     "exception": false,
     "start_time": "2023-03-06T19:41:32.071237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb01adf8913449f1970499a5789758aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c60e7bdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:39.456184Z",
     "iopub.status.busy": "2023-03-06T19:41:39.455526Z",
     "iopub.status.idle": "2023-03-06T19:41:39.465914Z",
     "shell.execute_reply": "2023-03-06T19:41:39.464769Z"
    },
    "id": "qlYFwx5XTCjE",
    "papermill": {
     "duration": 0.022404,
     "end_time": "2023-03-06T19:41:39.468354",
     "exception": false,
     "start_time": "2023-03-06T19:41:39.445950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForSpeechClassification(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (2): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (3): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (4): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (6): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Wav2Vec2ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.freeze_feature_extractor()\n",
    "model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9bac6",
   "metadata": {
    "id": "hpjqc0qlH5u0",
    "papermill": {
     "duration": 0.008729,
     "end_time": "2023-03-06T19:41:39.485983",
     "exception": false,
     "start_time": "2023-03-06T19:41:39.477254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸƒâ€â™€ï¸ training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbae9906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:39.505007Z",
     "iopub.status.busy": "2023-03-06T19:41:39.504428Z",
     "iopub.status.idle": "2023-03-06T19:41:40.709665Z",
     "shell.execute_reply": "2023-03-06T19:41:40.708531Z"
    },
    "id": "3frD5uDaR872",
    "papermill": {
     "duration": 1.218219,
     "end_time": "2023-03-06T19:41:40.712785",
     "exception": false,
     "start_time": "2023-03-06T19:41:39.494566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Union, Tuple, Optional\n",
    "from packaging import version\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import Trainer, is_apex_available, Wav2Vec2Processor, EvalPrediction, ProcessorMixin\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp \n",
    "    # Apex is a PyTorch add-on package from NVIDIA with capabilities for automatic mixed precision (AMP) and distributed training.\n",
    "    # https://www.ibm.com/docs/en/wmlce/1.6.1?topic=frameworks-getting-started-apex\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "  _is_native_amp_available = True\n",
    "  from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06c6305a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:40.733081Z",
     "iopub.status.busy": "2023-03-06T19:41:40.732419Z",
     "iopub.status.idle": "2023-03-06T19:41:40.743722Z",
     "shell.execute_reply": "2023-03-06T19:41:40.742653Z"
    },
    "id": "goTHilxaR3R5",
    "papermill": {
     "duration": 0.024066,
     "end_time": "2023-03-06T19:41:40.746177",
     "exception": false,
     "start_time": "2023-03-06T19:41:40.722111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "  processor: Wav2Vec2Processor\n",
    "  padding: Union[bool, str] = True\n",
    "  max_length: Optional[int] = None\n",
    "  max_length_labels: Optional[int] = None\n",
    "  pad_to_multiple_of: Optional[int] = None\n",
    "  pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "  def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "    input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n",
    "    label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "    d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "    batch = self.processor.pad(\n",
    "      input_features,\n",
    "      padding=self.padding,\n",
    "      max_length=self.max_length,\n",
    "      pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "      return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "    # print('batch', batch)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b6d03b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:40.765202Z",
     "iopub.status.busy": "2023-03-06T19:41:40.764898Z",
     "iopub.status.idle": "2023-03-06T19:41:40.770494Z",
     "shell.execute_reply": "2023-03-06T19:41:40.769393Z"
    },
    "id": "mqZbCzVJSmMP",
    "papermill": {
     "duration": 0.017806,
     "end_time": "2023-03-06T19:41:40.772837",
     "exception": false,
     "start_time": "2023-03-06T19:41:40.755031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d8c5fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:40.792628Z",
     "iopub.status.busy": "2023-03-06T19:41:40.791802Z",
     "iopub.status.idle": "2023-03-06T19:41:40.798391Z",
     "shell.execute_reply": "2023-03-06T19:41:40.797277Z"
    },
    "id": "5NxWSqMKGkRr",
    "papermill": {
     "duration": 0.018863,
     "end_time": "2023-03-06T19:41:40.800745",
     "exception": false,
     "start_time": "2023-03-06T19:41:40.781882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CTCTrainer(Trainer):\n",
    "  def training_step(self, processor: ProcessorMixin, inputs) -> torch.Tensor:\n",
    "    self.model.train()\n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "    with autocast():\n",
    "      loss = self.compute_loss(self.model, inputs)\n",
    "\n",
    "    if self.args.gradient_accumulation_steps > 1:\n",
    "      loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "    self.scaler.scale(loss).backward()\n",
    "\n",
    "    return loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26985941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:40.820672Z",
     "iopub.status.busy": "2023-03-06T19:41:40.820315Z",
     "iopub.status.idle": "2023-03-06T19:41:40.825454Z",
     "shell.execute_reply": "2023-03-06T19:41:40.824245Z"
    },
    "id": "eH9EZ7o3SWip",
    "papermill": {
     "duration": 0.018312,
     "end_time": "2023-03-06T19:41:40.827841",
     "exception": false,
     "start_time": "2023-03-06T19:41:40.809529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92a2c707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:40.847929Z",
     "iopub.status.busy": "2023-03-06T19:41:40.847001Z",
     "iopub.status.idle": "2023-03-06T19:41:45.759925Z",
     "shell.execute_reply": "2023-03-06T19:41:45.758804Z"
    },
    "id": "d1lLo-_vG8ih",
    "outputId": "801defb4-43ef-4fbe-bd31-db449cc18158",
    "papermill": {
     "duration": 4.925542,
     "end_time": "2023-03-06T19:41:45.762466",
     "exception": false,
     "start_time": "2023-03-06T19:41:40.836924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "trainer = CTCTrainer(\n",
    "  model=model,\n",
    "  data_collator=data_collator,\n",
    "  args=training_args,\n",
    "  compute_metrics=compute_metrics,\n",
    "  train_dataset=train_ds,\n",
    "  eval_dataset=test_ds,\n",
    "  tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cde6bd",
   "metadata": {
    "id": "QKJKwY4_IFCx",
    "papermill": {
     "duration": 0.009647,
     "end_time": "2023-03-06T19:41:45.782369",
     "exception": false,
     "start_time": "2023-03-06T19:41:45.772722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸ§ª experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e709beb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:41:45.804462Z",
     "iopub.status.busy": "2023-03-06T19:41:45.803306Z",
     "iopub.status.idle": "2023-03-06T21:22:58.746485Z",
     "shell.execute_reply": "2023-03-06T21:22:58.744944Z"
    },
    "id": "D9piW_jCHBav",
    "papermill": {
     "duration": 6072.958685,
     "end_time": "2023-03-06T21:22:58.750801",
     "exception": false,
     "start_time": "2023-03-06T19:41:45.792116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1476\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 369\n",
      "  Number of trainable parameters = 90766470\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msmeelock\u001b[0m (\u001b[33mtsinghua-ser\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20230306_194145-5b9sw243</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tsinghua-ser/huggingface/runs/5b9sw243' target=\"_blank\">visionary-violet-1</a></strong> to <a href='https://wandb.ai/tsinghua-ser/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tsinghua-ser/huggingface' target=\"_blank\">https://wandb.ai/tsinghua-ser/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tsinghua-ser/huggingface/runs/5b9sw243' target=\"_blank\">https://wandb.ai/tsinghua-ser/huggingface/runs/5b9sw243</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='369' max='369' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [369/369 1:40:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.873500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.792200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.856200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.775200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.678000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.747700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.777500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.777900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.768500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.728800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.705700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.750600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.674300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.725200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.797800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.702300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.676100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.629800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.800700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.671500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.768000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.787700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.618500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.634700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.636000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.741500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.865600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.712500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.834100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.765200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.747000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.709800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.720500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.760300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.652600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.683700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-10\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-10/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-10/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-10/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-20\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-20/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-20/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-20/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-30\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-30/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-30/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-30/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-40\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-40/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-40/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-40/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-50\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-50/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-50/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-50/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-60\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-60/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-60/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-60/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-70\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-70/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-70/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-70/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-80\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-80/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-80/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-80/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-90\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-90/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-90/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-90/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-100\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-100/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-100/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-110\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-110/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-110/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-110/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-120\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-120/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-120/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-120/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-130\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-130/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-130/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-130/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-110] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-140\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-140/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-140/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-140/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-150\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-150/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-150/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-150/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-160\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-160/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-160/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-160/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-170\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-170/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-170/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-170/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-180\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-180/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-180/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-180/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-190\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-190/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-190/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-190/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-170] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-200\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-200/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-180] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-210\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-210/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-210/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-210/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-190] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-220\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-220/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-220/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-220/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-230\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-230/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-230/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-230/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-210] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-240\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-240/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-240/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-240/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-220] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-250\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-250/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-250/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-250/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-230] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-260\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-260/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-260/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-260/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-270\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-270/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-270/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-270/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-280\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-280/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-280/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-280/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-290\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-290/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-290/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-290/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-270] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-300\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-300/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-310\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-310/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-310/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-310/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-290] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-320\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-320/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-320/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-320/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-330\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-330/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-330/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-330/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-310] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-340\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-340/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-340/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-340/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-350\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-350/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-350/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-350/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-330] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5904\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-360\n",
      "Configuration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-360/config.json\n",
      "Model weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-360/pytorch_model.bin\n",
      "Feature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-360/preprocessor_config.json\n",
      "Deleting older checkpoint [/content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-340] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=369, training_loss=1.7371736852134145, metrics={'train_runtime': 6072.429, 'train_samples_per_second': 0.243, 'train_steps_per_second': 0.061, 'total_flos': 1.0899090310430976e+17, 'train_loss': 1.7371736852134145, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6132.503741,
   "end_time": "2023-03-06T21:23:01.734803",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-06T19:40:49.231062",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "012a855fe4c04e4da81e5ce9f291c802": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1a8d5bdf0ad948fe9b5f4377cfc79ba2",
        "IPY_MODEL_a65c5b8269a0494a91fbf3fb194c0516",
        "IPY_MODEL_91bc1a7c5e8448cea674e40dfaa63ac3"
       ],
       "layout": "IPY_MODEL_b92e0a5290ac4d6ea453466bcc4ce5e5"
      }
     },
     "0437f50134154151b3cdd436fa4f9f0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_847c801003ae41568ede840d4eb8018f",
        "IPY_MODEL_45ceede8f7a64747871f0b03426a87e4",
        "IPY_MODEL_6a01571e76ad4935bc9080683a6300e0"
       ],
       "layout": "IPY_MODEL_4e2097b14eb14d409d770b913a8ef2d6"
      }
     },
     "0560863a63c84729b8a8a641aea8142a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "LabelModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "LabelView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5d1a66a5bf5347a89fb4e1bec649cffa",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_1ab7fdc0850f485aa8353bfa909b76a7",
       "value": ""
      }
     },
     "07981b8678e04e70bab451d70b00ecf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0b7ea945b09d4fcd9c0134edc46ea0f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_adcbe61356b74e16bb4814fe0cf94005",
        "IPY_MODEL_e804481bd0b14cd0b688c5b37de69ea2"
       ],
       "layout": "IPY_MODEL_e96d2bab37da4b1c9a31b9b9ae00174a"
      }
     },
     "0d72e50b7df746d9a432d95dc9ddc0e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7d605df5024a4befaf9fa3d87f6e76f4",
       "max": 291.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ffefe36522e64835b45fa0537e37c5e1",
       "value": 291.0
      }
     },
     "10039d486b26467e937d0e948a0f5b1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "114be29d53a44b5ea72a40b3ce7395af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "13a39de9ea6744cebc1d1320eb3fa400": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "159cb91d33234ab69d6d94cc43435824": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1a8d5bdf0ad948fe9b5f4377cfc79ba2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d1795e4f71444209867aa04c405f8910",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_4a796cd262d24287ab3e0e2d15e1b195",
       "value": "Downloading (â€¦)okenizer_config.json: 100%"
      }
     },
     "1ab7fdc0850f485aa8353bfa909b76a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1ed570c24efa47b48c94a7a1be16e7fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "24a93b22866d43769a211fa88145ca92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27dc1a3303ff4a04a03c525cf51d7281": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2bb4d3358e3f4178874a0eddf6cd1fc9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d50cbd5f6d64cf9be854564736d1ba2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5e456a09383b4de3a1088c0bc797d49b",
       "max": 1596.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a5957edb19bc4e8781acece12841067b",
       "value": 1596.0
      }
     },
     "374db2dbbe2049caaa3777c71c6332ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "39bf43b0cb14452f9d0e901563b9ed69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b3b65b9dd8d544d5a481f12d768e803e",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_159cb91d33234ab69d6d94cc43435824",
       "value": "Downloading (â€¦)rocessor_config.json: 100%"
      }
     },
     "45ceede8f7a64747871f0b03426a87e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9493faa1074846a9bab2ce3bdb92f611",
       "max": 85.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_97b5201e62894c8fa2d8eb88dfa7f224",
       "value": 85.0
      }
     },
     "4a796cd262d24287ab3e0e2d15e1b195": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4e2097b14eb14d409d770b913a8ef2d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5c24326535b340d4bd06b8421a1a5ac6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d1a66a5bf5347a89fb4e1bec649cffa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e456a09383b4de3a1088c0bc797d49b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6306c77e8fb340a080c790b168ed3493": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_39bf43b0cb14452f9d0e901563b9ed69",
        "IPY_MODEL_82f66954c27f4a51a9a36345afd72f48",
        "IPY_MODEL_bde01d90c54d4122bd52b86159ebaacd"
       ],
       "layout": "IPY_MODEL_8a1f9ddfa05d4ea1af380ecc03d74467"
      }
     },
     "63b4893d67214714aa9a0a915910315b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0560863a63c84729b8a8a641aea8142a",
        "IPY_MODEL_f0bdabe0ead94ba681a9e081b238236d"
       ],
       "layout": "IPY_MODEL_962f45db29f042a69fbf55505606f9f2"
      }
     },
     "648c969124bf408890d15e849e8d8e8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "64a724ec2231484596e42e9e6cee075f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8d84b4d34eb34d08b60052d430739ca9",
       "max": 377667514.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_648c969124bf408890d15e849e8d8e8a",
       "value": 377667514.0
      }
     },
     "66428a24aca0439390c73d23906be0a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "669c693edfab41d2bee1060a20ea187c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b8ab3464fb4440ba85b894807e8676c7",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a1b5f8df012b4bf493c681f3073664fd",
       "value": "Downloading (â€¦)&quot;pytorch_model.bin&quot;;: 100%"
      }
     },
     "6a01571e76ad4935bc9080683a6300e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ab3a955aea4b408aaadfa30c5dc97f19",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_dc8a90c300b94f279521b661a591d80b",
       "value": " 85.0/85.0 [00:00&lt;00:00, 4.51kB/s]"
      }
     },
     "6aac38277faf4133a4be7003d0a315b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "756b4940497a46639e8e1c453fc4a0cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7cfb0ff35d964d11b3ab7875638176fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6aac38277faf4133a4be7003d0a315b0",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_bd0e02011a5a4f5fa56011a20b3a180c",
       "value": " 378M/378M [00:05&lt;00:00, 68.2MB/s]"
      }
     },
     "7d605df5024a4befaf9fa3d87f6e76f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81741c4a91864f7f8cd5c6578f8702cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a3ad2645951946028d276384fe4aea12",
        "IPY_MODEL_0d72e50b7df746d9a432d95dc9ddc0e1",
        "IPY_MODEL_9a45bc13a64548e99d61a073ef6ba3f2"
       ],
       "layout": "IPY_MODEL_10039d486b26467e937d0e948a0f5b1a"
      }
     },
     "82f66954c27f4a51a9a36345afd72f48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fb2162a9e56f4d298da99340d910f657",
       "max": 159.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b13dd3c18c1e4e5e86db85bfe3622f25",
       "value": 159.0
      }
     },
     "847c801003ae41568ede840d4eb8018f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ece930c8946b48a3a9f08922740c8baf",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_07981b8678e04e70bab451d70b00ecf8",
       "value": "Downloading (â€¦)cial_tokens_map.json: 100%"
      }
     },
     "84967642b97b4d71b081131b81d27538": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9b188e6b5a244584b762647c634c118d",
        "IPY_MODEL_2d50cbd5f6d64cf9be854564736d1ba2",
        "IPY_MODEL_fab2abdb5e2a4087bdf814d1a1dd0be8"
       ],
       "layout": "IPY_MODEL_b0e4aaabff0540dcad45a4cd69c6c10b"
      }
     },
     "862c3e4b5b8c43839cde630212b5eb7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8a1f9ddfa05d4ea1af380ecc03d74467": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d84b4d34eb34d08b60052d430739ca9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8eeade19dfd241fbb0464846b6f615fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "91bc1a7c5e8448cea674e40dfaa63ac3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dab6e88d1ba949b4a92abe423b914829",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b3744a2bbdce40ba9e4a8d169d971e3f",
       "value": " 163/163 [00:00&lt;00:00, 5.83kB/s]"
      }
     },
     "9493faa1074846a9bab2ce3bdb92f611": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "962f45db29f042a69fbf55505606f9f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "97b5201e62894c8fa2d8eb88dfa7f224": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9a45bc13a64548e99d61a073ef6ba3f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8eeade19dfd241fbb0464846b6f615fb",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_c78bac14843848ac80c5c076e3e449ad",
       "value": " 291/291 [00:00&lt;00:00, 11.9kB/s]"
      }
     },
     "9b188e6b5a244584b762647c634c118d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_66428a24aca0439390c73d23906be0a6",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_abe920e3273a4db9a212c393c8fec389",
       "value": "Downloading (â€¦)lve/main/config.json: 100%"
      }
     },
     "a1b5f8df012b4bf493c681f3073664fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a1fdff5f436141f081fb5a6f892c2217": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a3ad2645951946028d276384fe4aea12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_facc77b3fe5142fcb51f0eeaf4139bcd",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a1fdff5f436141f081fb5a6f892c2217",
       "value": "Downloading (â€¦)olve/main/vocab.json: 100%"
      }
     },
     "a5957edb19bc4e8781acece12841067b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a65c5b8269a0494a91fbf3fb194c0516": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_24a93b22866d43769a211fa88145ca92",
       "max": 163.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f3bead0c84a942d89d80cf6104f088af",
       "value": 163.0
      }
     },
     "ab3a955aea4b408aaadfa30c5dc97f19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "abe920e3273a4db9a212c393c8fec389": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "adcbe61356b74e16bb4814fe0cf94005": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "LabelModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "LabelView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_13a39de9ea6744cebc1d1320eb3fa400",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_756b4940497a46639e8e1c453fc4a0cf",
       "value": ""
      }
     },
     "b0e4aaabff0540dcad45a4cd69c6c10b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b13dd3c18c1e4e5e86db85bfe3622f25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b3744a2bbdce40ba9e4a8d169d971e3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b3b65b9dd8d544d5a481f12d768e803e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8ab3464fb4440ba85b894807e8676c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b92e0a5290ac4d6ea453466bcc4ce5e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bd0e02011a5a4f5fa56011a20b3a180c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bde01d90c54d4122bd52b86159ebaacd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_deb287a0f2ef446ea25471982d55b609",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_862c3e4b5b8c43839cde630212b5eb7e",
       "value": " 159/159 [00:00&lt;00:00, 5.52kB/s]"
      }
     },
     "c78bac14843848ac80c5c076e3e449ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cb01adf8913449f1970499a5789758aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_669c693edfab41d2bee1060a20ea187c",
        "IPY_MODEL_64a724ec2231484596e42e9e6cee075f",
        "IPY_MODEL_7cfb0ff35d964d11b3ab7875638176fe"
       ],
       "layout": "IPY_MODEL_2bb4d3358e3f4178874a0eddf6cd1fc9"
      }
     },
     "d1795e4f71444209867aa04c405f8910": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d469d99665a542e28502713f63ad4312": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dab6e88d1ba949b4a92abe423b914829": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc8a90c300b94f279521b661a591d80b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "deb287a0f2ef446ea25471982d55b609": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e804481bd0b14cd0b688c5b37de69ea2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_27dc1a3303ff4a04a03c525cf51d7281",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_374db2dbbe2049caaa3777c71c6332ef",
       "value": 0.0
      }
     },
     "e96d2bab37da4b1c9a31b9b9ae00174a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ece930c8946b48a3a9f08922740c8baf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0bdabe0ead94ba681a9e081b238236d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d469d99665a542e28502713f63ad4312",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1ed570c24efa47b48c94a7a1be16e7fa",
       "value": 0.0
      }
     },
     "f3bead0c84a942d89d80cf6104f088af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fab2abdb5e2a4087bdf814d1a1dd0be8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5c24326535b340d4bd06b8421a1a5ac6",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_114be29d53a44b5ea72a40b3ce7395af",
       "value": " 1.60k/1.60k [00:00&lt;00:00, 82.5kB/s]"
      }
     },
     "facc77b3fe5142fcb51f0eeaf4139bcd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb2162a9e56f4d298da99340d910f657": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ffefe36522e64835b45fa0537e37c5e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
