{"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["JWxbRKGh3RzQ","w9lfgKcGHlQF"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard","accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## init","metadata":{"id":"JWxbRKGh3RzQ"}},{"cell_type":"code","source":"%pip install -qqq transformers torchaudio datasets wandb","metadata":{"id":"RCAHK5kxBSHF","execution":{"iopub.status.busy":"2023-03-12T18:22:01.795850Z","iopub.execute_input":"2023-03-12T18:22:01.796378Z","iopub.status.idle":"2023-03-12T18:22:12.392947Z","shell.execute_reply.started":"2023-03-12T18:22:01.796331Z","shell.execute_reply":"2023-03-12T18:22:12.391619Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# env variables\n# the datapath is actually \"/kaggle/input/iemocapfullrelease\" but we use a symlink to get to the dataset\nDATA_PATH = \"/kaggle/working\"\n\n# we need a symling because torchaudio.datasets.IEMOCAP adds a \"/IEMOCAP\" to the data path, but the dataset is at /kaggle/input/iemocapfullrelease/IEMOCAP_full_release and the directory is read-only\n!ln -s /kaggle/input/iemocapfullrelease/IEMOCAP_full_release /kaggle/working/IEMOCAP\n\n# log model checkpoints\n%env WANDB_LOG_MODEL=\"checkpoint\"\n\n# log gradiens and model parameters\n%env WANDB_WATCH=\"all\"","metadata":{"id":"1IyiqteO2ejj","execution":{"iopub.status.busy":"2023-03-12T18:22:12.395680Z","iopub.execute_input":"2023-03-12T18:22:12.396079Z","iopub.status.idle":"2023-03-12T18:22:13.355594Z","shell.execute_reply.started":"2023-03-12T18:22:12.396037Z","shell.execute_reply":"2023-03-12T18:22:13.354064Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"env: WANDB_LOG_MODEL=\"checkpoint\"\nenv: WANDB_WATCH=\"all\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"wandb-api-token\") \nwandb.login(key=token)","metadata":{"id":"jLWXtxDA2heR","execution":{"iopub.status.busy":"2023-03-12T18:22:13.356957Z","iopub.execute_input":"2023-03-12T18:22:13.357324Z","iopub.status.idle":"2023-03-12T18:22:16.447212Z","shell.execute_reply.started":"2023-03-12T18:22:13.357280Z","shell.execute_reply":"2023-03-12T18:22:16.446151Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Use wav2vec2 for speech emotion recognition on IEMOCAP dataset**\n---\n- 🚀 **objective**: run wav2vec2 as a feature extractor on IEMOCAP dataset, requires the data preprocessing of IEMOCAP dataset  \n- 🧯 **models**: wav2vec2\n- 📚 **dataset**: IEMOCAP\n\n\nResources\n- inspired by https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb#scrollTo=Fv62ShDsH5DZ","metadata":{"id":"ebaFEkvH3R6W"}},{"cell_type":"markdown","source":"## ⚙️ configuration","metadata":{"id":"VTM5mAoeArCU"}},{"cell_type":"code","source":"import numpy as np\n\nfrom transformers import TrainingArguments\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n\nimport torch","metadata":{"id":"E5slPz53BPEB","execution":{"iopub.status.busy":"2023-03-12T18:22:16.451398Z","iopub.execute_input":"2023-03-12T18:22:16.453503Z","iopub.status.idle":"2023-03-12T18:22:26.259748Z","shell.execute_reply.started":"2023-03-12T18:22:16.453468Z","shell.execute_reply":"2023-03-12T18:22:26.258609Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"facebook/wav2vec2-base-960h\"\nfeature_to_idx = {key: i for i, key in enumerate([\"wav\", \"sampling_rate\", \"filename\", \"label\", \"speaker\"])}\nlabel_list = [\"neu\", \"hap\", \"ang\", \"sad\", \"exc\", \"fru\"]\nnum_labels = len(label_list)\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for i, label in enumerate(label_list)}\n\npooling_mode = \"max\"\ntest_split_size = 0.2\ntarget_sampling_rate = 16000\n\nDEBUG_SIZE = 0.1 # percentage of the whole dataset\n\nmetrics = {\n  \"unweighted_accuracy\": accuracy_score,\n  \"weighted_accuracy\": balanced_accuracy_score,\n  \"micro_f1\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"micro\"),\n  \"macro_f1\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")\n}","metadata":{"id":"c9FAPgXIKa04","execution":{"iopub.status.busy":"2023-03-12T18:22:26.261320Z","iopub.execute_input":"2023-03-12T18:22:26.262792Z","iopub.status.idle":"2023-03-12T18:22:26.272311Z","shell.execute_reply.started":"2023-03-12T18:22:26.262759Z","shell.execute_reply":"2023-03-12T18:22:26.271254Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# training parameters\ntraining_args = TrainingArguments(\n    output_dir=\"/content/wav2vec2-iemocap-speech-emotion-recognition\",\n    label_names=label_list,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=1,\n    evaluation_strategy=\"steps\", # should enable do_eval\n    num_train_epochs=1.0,\n    learning_rate=1e-4,\n    fp16=torch.cuda.is_available(), # whether to use fp16 16-bit (mixed) precision training instead of 32-bit training\n    save_steps=10,\n    eval_steps=10,\n    logging_steps=10,\n    report_to=\"wandb\",\n    half_precision_backend=\"auto\", # shoud be 'cuda_amp' half precision backend\n    gradient_checkpointing=True, # use gradient checkpointing to save memory at the expense of slower backward pass\n)","metadata":{"id":"OeMVreG3-6qs","execution":{"iopub.status.busy":"2023-03-12T18:22:26.274046Z","iopub.execute_input":"2023-03-12T18:22:26.274805Z","iopub.status.idle":"2023-03-12T18:22:26.357370Z","shell.execute_reply.started":"2023-03-12T18:22:26.274760Z","shell.execute_reply":"2023-03-12T18:22:26.356377Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## 📚 data\n- ~torchaudio implemented a `dataset` to load IEMOCAP. Later in the script, we train the model with a `Trainer` from hugginface, therefore we prefer translating the pytorch dataset into a `transformers.Dataset` for convenience and compatibility.~\n- the Trainer class expects an argument `train_dataset` to be of type torch.utils.data.Dataset (see [documentation](https://huggingface.co/docs/transformers/main_classes/trainer)) --> we use a torch dataset instead of a Hugginface dataset","metadata":{"id":"w9lfgKcGHlQF"}},{"cell_type":"code","source":"# https://pytorch.org/audio/master/generated/torchaudio.datasets.IEMOCAP.html\nfrom torchaudio.datasets import IEMOCAP\n\nfrom transformers import Wav2Vec2Processor\n\nimport torch\nfrom torch.utils.data import random_split, Dataset, DataLoader, SubsetRandomSampler","metadata":{"id":"63mPgt-MR8p0","execution":{"iopub.status.busy":"2023-03-12T18:22:26.358948Z","iopub.execute_input":"2023-03-12T18:22:26.359692Z","iopub.status.idle":"2023-03-12T18:22:27.165766Z","shell.execute_reply.started":"2023-03-12T18:22:26.359655Z","shell.execute_reply":"2023-03-12T18:22:27.164683Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\ntarget_sampling_rate = processor.feature_extractor.sampling_rate","metadata":{"id":"3h-qBGonTgmf","execution":{"iopub.status.busy":"2023-03-12T18:22:27.167251Z","iopub.execute_input":"2023-03-12T18:22:27.169134Z","iopub.status.idle":"2023-03-12T18:22:30.281764Z","shell.execute_reply.started":"2023-03-12T18:22:27.169076Z","shell.execute_reply":"2023-03-12T18:22:30.280550Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9928e6d810844ba1b7ab5ead3184f5b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84bed56164b4f48855a5c2adc8f9c6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05e496f1709d4eac9d932d1bdce735b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a260383877544a4b3f1fe75d5b1e71f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a86a890edc35430a986d387865eeac62"}},"metadata":{}}]},{"cell_type":"code","source":"class CustomIEMOCAP(Dataset):\n  def __init__(self, data, processor):\n    self.data = data\n    self.processor = processor\n\n  def __getitem__(self, index):\n    wav, _, _, label, _ = self.data[index]\n    inputs = self.processor(wav.squeeze(), sampling_rate=target_sampling_rate)\n    inputs[\"labels\"] = label2id[label]\n\n    return inputs\n\n  def __len__(self):\n    return len(self.data)","metadata":{"id":"lEkXSO7-wI2t","execution":{"iopub.status.busy":"2023-03-12T18:22:30.283560Z","iopub.execute_input":"2023-03-12T18:22:30.284295Z","iopub.status.idle":"2023-03-12T18:22:30.291804Z","shell.execute_reply.started":"2023-03-12T18:22:30.284257Z","shell.execute_reply":"2023-03-12T18:22:30.290536Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"iemocap = IEMOCAP(root=DATA_PATH) # in function, path = root / \"IEMOCAP\"\niemocap = torch.utils.data.Subset(iemocap, range(int(DEBUG_SIZE * len(iemocap)))) # DEBUG\n\ndataset = CustomIEMOCAP(data=iemocap, processor=processor)\ntrain_ds, test_ds = random_split(dataset, [1-test_split_size, test_split_size], generator=torch.Generator().manual_seed(42))\n\ndataset[0]","metadata":{"id":"TkLW4D2ZwYCl","execution":{"iopub.status.busy":"2023-03-12T18:22:30.296738Z","iopub.execute_input":"2023-03-12T18:22:30.297481Z","iopub.status.idle":"2023-03-12T18:22:34.616518Z","shell.execute_reply.started":"2023-03-12T18:22:30.297441Z","shell.execute_reply":"2023-03-12T18:22:34.615337Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'input_values': [array([-0.38823238, -0.3835091 , -0.2937664 , ..., -0.20402369,\n       -0.24417175, -0.32210618], dtype=float32)], 'labels': 0}"},"metadata":{}}]},{"cell_type":"markdown","source":"## 🚜 model definition","metadata":{"id":"_DZWd3CGHnnc"}},{"cell_type":"code","source":"from transformers import Wav2Vec2Model, Wav2Vec2PreTrainedModel\nfrom transformers import AutoConfig\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss","metadata":{"id":"OAJrSE-vR4OO","execution":{"iopub.status.busy":"2023-03-12T18:22:34.617940Z","iopub.execute_input":"2023-03-12T18:22:34.619409Z","iopub.status.idle":"2023-03-12T18:22:34.730462Z","shell.execute_reply.started":"2023-03-12T18:22:34.619366Z","shell.execute_reply":"2023-03-12T18:22:34.729476Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# model configuration\nconfig = AutoConfig.from_pretrained(\n  model_name_or_path,\n  num_labels=num_labels,\n  label2id=label2id,\n  id2label=id2label,\n  finetuning_task=\"wav2vec2_clf\",\n)\nsetattr(config, 'pooling_mode', pooling_mode)\nconfig","metadata":{"id":"a-JbmAoyT6v0","execution":{"iopub.status.busy":"2023-03-12T18:22:34.732209Z","iopub.execute_input":"2023-03-12T18:22:34.732591Z","iopub.status.idle":"2023-03-12T18:22:35.011601Z","shell.execute_reply.started":"2023-03-12T18:22:34.732552Z","shell.execute_reply":"2023-03-12T18:22:35.010293Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Wav2Vec2Config {\n  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n  \"activation_dropout\": 0.1,\n  \"adapter_kernel_size\": 3,\n  \"adapter_stride\": 2,\n  \"add_adapter\": false,\n  \"apply_spec_augment\": true,\n  \"architectures\": [\n    \"Wav2Vec2ForCTC\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 1,\n  \"classifier_proj_size\": 256,\n  \"codevector_dim\": 256,\n  \"contrastive_logits_temperature\": 0.1,\n  \"conv_bias\": false,\n  \"conv_dim\": [\n    512,\n    512,\n    512,\n    512,\n    512,\n    512,\n    512\n  ],\n  \"conv_kernel\": [\n    10,\n    3,\n    3,\n    3,\n    3,\n    2,\n    2\n  ],\n  \"conv_stride\": [\n    5,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2\n  ],\n  \"ctc_loss_reduction\": \"sum\",\n  \"ctc_zero_infinity\": false,\n  \"diversity_loss_weight\": 0.1,\n  \"do_stable_layer_norm\": false,\n  \"eos_token_id\": 2,\n  \"feat_extract_activation\": \"gelu\",\n  \"feat_extract_dropout\": 0.0,\n  \"feat_extract_norm\": \"group\",\n  \"feat_proj_dropout\": 0.1,\n  \"feat_quantizer_dropout\": 0.0,\n  \"final_dropout\": 0.1,\n  \"finetuning_task\": \"wav2vec2_clf\",\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout\": 0.1,\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"neu\",\n    \"1\": \"hap\",\n    \"2\": \"ang\",\n    \"3\": \"sad\",\n    \"4\": \"exc\",\n    \"5\": \"fru\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"ang\": 2,\n    \"exc\": 4,\n    \"fru\": 5,\n    \"hap\": 1,\n    \"neu\": 0,\n    \"sad\": 3\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"layerdrop\": 0.1,\n  \"mask_feature_length\": 10,\n  \"mask_feature_min_masks\": 0,\n  \"mask_feature_prob\": 0.0,\n  \"mask_time_length\": 10,\n  \"mask_time_min_masks\": 2,\n  \"mask_time_prob\": 0.05,\n  \"model_type\": \"wav2vec2\",\n  \"num_adapter_layers\": 3,\n  \"num_attention_heads\": 12,\n  \"num_codevector_groups\": 2,\n  \"num_codevectors_per_group\": 320,\n  \"num_conv_pos_embedding_groups\": 16,\n  \"num_conv_pos_embeddings\": 128,\n  \"num_feat_extract_layers\": 7,\n  \"num_hidden_layers\": 12,\n  \"num_negatives\": 100,\n  \"output_hidden_size\": 768,\n  \"pad_token_id\": 0,\n  \"pooling_mode\": \"max\",\n  \"proj_codevector_dim\": 256,\n  \"tdnn_dilation\": [\n    1,\n    2,\n    3,\n    1,\n    1\n  ],\n  \"tdnn_dim\": [\n    512,\n    512,\n    512,\n    512,\n    1500\n  ],\n  \"tdnn_kernel\": [\n    5,\n    3,\n    3,\n    1,\n    1\n  ],\n  \"transformers_version\": \"4.26.1\",\n  \"use_weighted_layer_sum\": false,\n  \"vocab_size\": 32,\n  \"xvector_output_dim\": 512\n}"},"metadata":{}}]},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union, Tuple\nfrom transformers.file_utils import ModelOutput\n\n@dataclass\nclass SpeechClassifierOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None","metadata":{"id":"54M_I54lQ-QJ","execution":{"iopub.status.busy":"2023-03-12T18:22:35.013057Z","iopub.execute_input":"2023-03-12T18:22:35.013933Z","iopub.status.idle":"2023-03-12T18:22:35.021570Z","shell.execute_reply.started":"2023-03-12T18:22:35.013903Z","shell.execute_reply":"2023-03-12T18:22:35.020293Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class Wav2Vec2ClassificationHead(nn.Module):\n  \"\"\"Head for wav2vec classification task.\"\"\"\n\n  def __init__(self, config):\n    super().__init__()\n    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n    self.dropout = nn.Dropout(config.final_dropout)\n    self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n  def forward(self, features, **kwargs):\n    x = features\n    x = self.dropout(x)\n    x = self.dense(x)\n    x = torch.tanh(x)\n    x = self.dropout(x)\n    x = self.out_proj(x)\n    return x\n\n\nclass Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n  def __init__(self, config):\n    super().__init__(config)\n    self.num_labels = config.num_labels\n    self.pooling_mode = config.pooling_mode\n    self.config = config\n\n    self.wav2vec2 = Wav2Vec2Model(config)\n    self.classifier = Wav2Vec2ClassificationHead(config)\n\n    self.init_weights()\n\n  def freeze_feature_extractor(self):\n    self.wav2vec2.feature_extractor._freeze_parameters()\n\n  def merged_strategy(\n      \n      self,\n      hidden_states,\n      mode=\"mean\"\n  ):\n    if mode == \"mean\":\n        outputs = torch.mean(hidden_states, dim=1)\n    elif mode == \"sum\":\n        outputs = torch.sum(hidden_states, dim=1)\n    elif mode == \"max\":\n        outputs = torch.max(hidden_states, dim=1)[0]\n    else:\n        raise Exception(\n            \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n\n    return outputs\n\n  def forward(\n      self,\n      input_values,\n      attention_mask=None,\n      output_attentions=None,\n      output_hidden_states=None,\n      return_dict=None,\n      labels=None,\n  ):\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n    outputs = self.wav2vec2(\n        input_values,\n        attention_mask=attention_mask,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    hidden_states = outputs[0]\n    hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n    logits = self.classifier(hidden_states)\n\n    loss = None\n    if labels is not None:\n      loss_fct = CrossEntropyLoss()\n      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n    if not return_dict:\n      output = (logits,) + outputs[2:]\n      return ((loss,) + output) if loss is not None else output\n\n    return SpeechClassifierOutput(\n      loss=loss,\n      logits=logits,\n      hidden_states=outputs.hidden_states,\n      attentions=outputs.attentions,\n    )","metadata":{"id":"P1HzzCqPAuSH","execution":{"iopub.status.busy":"2023-03-12T18:22:35.023622Z","iopub.execute_input":"2023-03-12T18:22:35.024095Z","iopub.status.idle":"2023-03-12T18:22:35.040365Z","shell.execute_reply.started":"2023-03-12T18:22:35.024057Z","shell.execute_reply":"2023-03-12T18:22:35.039268Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = Wav2Vec2ForSpeechClassification.from_pretrained(\n    model_name_or_path,\n    config=config,\n)","metadata":{"id":"C7aPeO-fGBt-","execution":{"iopub.status.busy":"2023-03-12T18:22:35.041850Z","iopub.execute_input":"2023-03-12T18:22:35.042949Z","iopub.status.idle":"2023-03-12T18:22:38.698641Z","shell.execute_reply.started":"2023-03-12T18:22:35.042907Z","shell.execute_reply":"2023-03-12T18:22:38.697698Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"945036b070df49f19024cdbe8cc55ac5"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.bias', 'lm_head.weight']\n- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.freeze_feature_extractor()\nmodel ","metadata":{"id":"qlYFwx5XTCjE","execution":{"iopub.status.busy":"2023-03-12T18:22:38.700184Z","iopub.execute_input":"2023-03-12T18:22:38.700741Z","iopub.status.idle":"2023-03-12T18:22:38.712977Z","shell.execute_reply.started":"2023-03-12T18:22:38.700702Z","shell.execute_reply":"2023-03-12T18:22:38.711171Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Wav2Vec2ForSpeechClassification(\n  (wav2vec2): Wav2Vec2Model(\n    (feature_extractor): Wav2Vec2FeatureEncoder(\n      (conv_layers): ModuleList(\n        (0): Wav2Vec2GroupNormConvLayer(\n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n          (activation): GELUActivation()\n          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n        )\n        (1): Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (2): Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (3): Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (4): Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (5): Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (6): Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (feature_projection): Wav2Vec2FeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): Wav2Vec2Encoder(\n      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n        (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n        (padding): Wav2Vec2SamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (classifier): Wav2Vec2ClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## 🏃‍♀️ training routine","metadata":{"id":"hpjqc0qlH5u0"}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Dict, Union, Tuple, Optional\nfrom packaging import version\nimport numpy as np\n\nimport torch\nfrom torch import nn\n\nfrom transformers import Trainer, is_apex_available, Wav2Vec2Processor, EvalPrediction\n\nif is_apex_available():\n    from apex import amp \n    # Apex is a PyTorch add-on package from NVIDIA with capabilities for automatic mixed precision (AMP) and distributed training.\n    # https://www.ibm.com/docs/en/wmlce/1.6.1?topic=frameworks-getting-started-apex\n\nif version.parse(torch.__version__) >= version.parse(\"1.6\"):\n  _is_native_amp_available = True\n  from torch.cuda.amp import autocast","metadata":{"id":"3frD5uDaR872","execution":{"iopub.status.busy":"2023-03-12T18:22:38.715697Z","iopub.execute_input":"2023-03-12T18:22:38.715997Z","iopub.status.idle":"2023-03-12T18:22:39.127487Z","shell.execute_reply.started":"2023-03-12T18:22:38.715957Z","shell.execute_reply":"2023-03-12T18:22:39.126486Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorCTCWithPadding:\n  processor: Wav2Vec2Processor\n  padding: Union[bool, str] = True\n  max_length: Optional[int] = None\n  max_length_labels: Optional[int] = None\n  pad_to_multiple_of: Optional[int] = None\n  pad_to_multiple_of_labels: Optional[int] = None\n\n  def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n    input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n    label_features = [feature[\"labels\"] for feature in features]\n\n    d_type = torch.long if isinstance(label_features[0], int) else torch.float\n\n    batch = self.processor.pad(\n      input_features,\n      padding=self.padding,\n      max_length=self.max_length,\n      pad_to_multiple_of=self.pad_to_multiple_of,\n      return_tensors=\"pt\",\n    )\n\n    batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n    # print('batch', batch)\n    return batch","metadata":{"id":"goTHilxaR3R5","execution":{"iopub.status.busy":"2023-03-12T18:22:39.128890Z","iopub.execute_input":"2023-03-12T18:22:39.129656Z","iopub.status.idle":"2023-03-12T18:22:39.138991Z","shell.execute_reply.started":"2023-03-12T18:22:39.129617Z","shell.execute_reply":"2023-03-12T18:22:39.137929Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    preds = np.argmax(preds, axis=1)\n    # (preds == p.label_ids).astype(np.float32).mean().item()\n    return {k: metric(preds, p.label_ids) for k, metric in metrics.items()}","metadata":{"id":"mqZbCzVJSmMP","execution":{"iopub.status.busy":"2023-03-12T18:22:39.140888Z","iopub.execute_input":"2023-03-12T18:22:39.141592Z","iopub.status.idle":"2023-03-12T18:22:39.150541Z","shell.execute_reply.started":"2023-03-12T18:22:39.141551Z","shell.execute_reply":"2023-03-12T18:22:39.149410Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class CTCTrainer(Trainer):\n  def training_step(self, model, inputs) -> torch.Tensor:\n    model.train()\n    inputs = self._prepare_inputs(inputs)\n\n    with autocast():\n      # loss = self.compute_loss(model, inputs)\n      loss = model(**inputs).get(\"loss\")\n\n    if self.args.gradient_accumulation_steps > 1:\n      loss = loss / self.args.gradient_accumulation_steps\n\n    self.scaler.scale(loss).backward()\n\n    return loss.detach()\n  \n  def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys) -> torch.Tensor: \n    model.eval()\n    inputs = self._prepare_inputs(inputs)\n\n    labels = inputs.get(\"labels\")\n\n    with autocast():\n      outputs = model(**inputs)\n      logits = outputs.get(\"logits\")\n      loss = outputs.get(\"loss\")\n\n    if self.args.gradient_accumulation_steps > 1:\n      loss = loss / self.args.gradient_accumulation_steps\n\n    self.scaler.scale(loss).backward()\n\n    with torch.no_grad():\n      torch.cuda.empty_cache()\n\n    if prediction_loss_only:\n      return loss.detach()\n    return (loss.detach(), logits.detach(), labels.detach())","metadata":{"id":"5NxWSqMKGkRr","execution":{"iopub.status.busy":"2023-03-12T18:22:39.152461Z","iopub.execute_input":"2023-03-12T18:22:39.152898Z","iopub.status.idle":"2023-03-12T18:22:39.162640Z","shell.execute_reply.started":"2023-03-12T18:22:39.152863Z","shell.execute_reply":"2023-03-12T18:22:39.161411Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","metadata":{"id":"eH9EZ7o3SWip","execution":{"iopub.status.busy":"2023-03-12T18:22:39.164148Z","iopub.execute_input":"2023-03-12T18:22:39.164813Z","iopub.status.idle":"2023-03-12T18:22:39.175298Z","shell.execute_reply.started":"2023-03-12T18:22:39.164769Z","shell.execute_reply":"2023-03-12T18:22:39.174263Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\ntrainer = CTCTrainer(\n  model=model,\n  data_collator=data_collator,\n  args=training_args,\n  compute_metrics=compute_metrics,\n  train_dataset=train_ds,\n  eval_dataset=test_ds,\n  tokenizer=processor.feature_extractor,\n)","metadata":{"id":"d1lLo-_vG8ih","execution":{"iopub.status.busy":"2023-03-12T18:22:39.176936Z","iopub.execute_input":"2023-03-12T18:22:39.177331Z","iopub.status.idle":"2023-03-12T18:22:43.478812Z","shell.execute_reply.started":"2023-03-12T18:22:39.177295Z","shell.execute_reply":"2023-03-12T18:22:43.477688Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Using cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 🧪 experiments","metadata":{"id":"QKJKwY4_IFCx"}},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"D9piW_jCHBav","execution":{"iopub.status.busy":"2023-03-12T18:22:43.480231Z","iopub.execute_input":"2023-03-12T18:22:43.480710Z","iopub.status.idle":"2023-03-12T18:27:37.032340Z","shell.execute_reply.started":"2023-03-12T18:22:43.480667Z","shell.execute_reply":"2023-03-12T18:27:37.030618Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 591\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 148\n  Number of trainable parameters = 90766470\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msmeelock\u001b[0m (\u001b[33mtsinghua-ser\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.11 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230312_182243-7z9xa9d3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tsinghua-ser/huggingface/runs/7z9xa9d3' target=\"_blank\">solar-surf-5</a></strong> to <a href='https://wandb.ai/tsinghua-ser/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tsinghua-ser/huggingface' target=\"_blank\">https://wandb.ai/tsinghua-ser/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tsinghua-ser/huggingface/runs/7z9xa9d3' target=\"_blank\">https://wandb.ai/tsinghua-ser/huggingface/runs/7z9xa9d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='148' max='148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [148/148 04:17, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Unweighted Accuracy</th>\n      <th>Weighted Accuracy</th>\n      <th>Micro F1</th>\n      <th>Macro F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.864300</td>\n      <td>1.742726</td>\n      <td>0.272109</td>\n      <td>0.288866</td>\n      <td>0.272109</td>\n      <td>0.117898</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.616000</td>\n      <td>1.713967</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.627000</td>\n      <td>1.702909</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.536000</td>\n      <td>1.668560</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.723100</td>\n      <td>1.656702</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.740100</td>\n      <td>1.646066</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.700200</td>\n      <td>1.627023</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.807000</td>\n      <td>1.647524</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.639700</td>\n      <td>1.649417</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.648200</td>\n      <td>1.623924</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.661400</td>\n      <td>1.568791</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.319728</td>\n      <td>0.080756</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.694900</td>\n      <td>1.574398</td>\n      <td>0.326531</td>\n      <td>0.660959</td>\n      <td>0.326531</td>\n      <td>0.090978</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.661300</td>\n      <td>1.539388</td>\n      <td>0.367347</td>\n      <td>0.567883</td>\n      <td>0.367347</td>\n      <td>0.145349</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.551500</td>\n      <td>1.531868</td>\n      <td>0.360544</td>\n      <td>0.517883</td>\n      <td>0.360544</td>\n      <td>0.137597</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-10\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-10/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-10/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-10/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-20\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-20/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-20/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-20/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-30\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-30/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-30/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-30/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-40\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-40/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-40/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-40/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-50\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-50/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-50/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-50/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-60\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-60/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-60/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-60/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-70\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-70/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-70/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-70/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-80\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-80/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-80/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-80/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-90\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-90/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-90/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-90/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-100\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-100/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-100/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-100/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-110\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-110/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-110/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-110/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-120\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-120/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-120/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-120/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-130\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-130/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-130/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-130/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 147\n  Batch size = 4\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n  warnings.warn(\"y_pred contains classes not in y_true\")\nSaving model checkpoint to /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-140\nConfiguration saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-140/config.json\nModel weights saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-140/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-iemocap-speech-emotion-recognition/checkpoint-140/preprocessor_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=148, training_loss=1.6819103344066724, metrics={'train_runtime': 292.9486, 'train_samples_per_second': 2.017, 'train_steps_per_second': 0.505, 'total_flos': 4.258881998163005e+16, 'train_loss': 1.6819103344066724, 'epoch': 1.0})"},"metadata":{}}]}]}