{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## init","metadata":{"id":"JWxbRKGh3RzQ"}},{"cell_type":"code","source":"%pip install -qqq transformers torchaudio datasets wandb","metadata":{"id":"RCAHK5kxBSHF","outputId":"ee31a324-495c-4108-a707-82e4b3b720bd","execution":{"iopub.status.busy":"2023-03-19T22:13:33.818788Z","iopub.execute_input":"2023-03-19T22:13:33.819266Z","iopub.status.idle":"2023-03-19T22:13:45.225863Z","shell.execute_reply.started":"2023-03-19T22:13:33.819214Z","shell.execute_reply":"2023-03-19T22:13:45.224490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# env variables\n# the datapath is actually \"/kaggle/input/iemocapfullrelease\" but we use a symlink to get to the dataset\nDATA_PATH = \"/kaggle/working\"\nOUTPUT_DIR = \"/kaggle/working\"\n\n# we need a symling because torchaudio.datasets.IEMOCAP adds a \"/IEMOCAP\" to the data path, but the dataset is at /kaggle/input/iemocapfullrelease/IEMOCAP_full_release and the directory is read-only\n!ln -s /kaggle/input/iemocapfullrelease/IEMOCAP_full_release /kaggle/working/IEMOCAP\n\n%env WANDB_WATCH=all\n%env WANDB_LOG_MODEL=checkpoint","metadata":{"id":"u3xkTBGt2cwf","execution":{"iopub.status.busy":"2023-03-19T22:13:45.228651Z","iopub.execute_input":"2023-03-19T22:13:45.229035Z","iopub.status.idle":"2023-03-19T22:13:46.164618Z","shell.execute_reply.started":"2023-03-19T22:13:45.228971Z","shell.execute_reply":"2023-03-19T22:13:46.163199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"wandb-api-token\") \nwandb.login(key=token)","metadata":{"id":"FzihRGsm7LxN","outputId":"50c8cb0a-964e-46ba-e7ed-af340be6e88e","execution":{"iopub.status.busy":"2023-03-19T22:17:15.078983Z","iopub.execute_input":"2023-03-19T22:17:15.079963Z","iopub.status.idle":"2023-03-19T22:17:17.803341Z","shell.execute_reply.started":"2023-03-19T22:17:15.079911Z","shell.execute_reply":"2023-03-19T22:17:17.802114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Use OpenAI's whisper for speech emotion recognition on IEMOCAP dataset**\n---\n- ðŸš€ **objective**: run whisper as a feature extractor on IEMOCAP dataset, requires the data preprocessing of IEMOCAP dataset  \n- ðŸ§¯ **models**: whisper\n- ðŸ“š **dataset**: IEMOCAP\n\nWhisper model card in HuggingFace https://huggingface.co/docs/transformers/model_doc/whisper","metadata":{"id":"ebaFEkvH3R6W"}},{"cell_type":"markdown","source":"## âš™ï¸ configuration","metadata":{"id":"VTM5mAoeArCU"}},{"cell_type":"code","source":"import numpy as np\n\nfrom transformers import TrainingArguments\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n\nimport torch","metadata":{"id":"E5slPz53BPEB","execution":{"iopub.status.busy":"2023-03-19T22:17:22.835565Z","iopub.execute_input":"2023-03-19T22:17:22.835955Z","iopub.status.idle":"2023-03-19T22:17:33.727633Z","shell.execute_reply.started":"2023-03-19T22:17:22.835913Z","shell.execute_reply":"2023-03-19T22:17:33.726455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"openai/whisper-base\"\nfeature_to_idx = {key: i for i, key in enumerate([\"wav\", \"sampling_rate\", \"filename\", \"label\", \"speaker\"])}\nlabel_list = [\"neu\", \"hap\", \"ang\", \"sad\", \"exc\", \"fru\"]\nnum_labels = len(label_list)\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for i, label in enumerate(label_list)}\n\npooling_mode = \"max\"\ntest_split_size = 0.2\ntarget_sampling_rate = 16000\n\nDEBUG_SIZE = 1 # percentage of the whole dataset\n\nkeep_n_encoder_layers = 4\n\nmetrics = {\n  \"unweighted_accuracy\": accuracy_score,\n  \"weighted_accuracy\": balanced_accuracy_score,\n  \"micro_f1\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"micro\"),\n  \"macro_f1\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")\n}","metadata":{"id":"c9FAPgXIKa04","execution":{"iopub.status.busy":"2023-03-19T22:17:33.730376Z","iopub.execute_input":"2023-03-19T22:17:33.731557Z","iopub.status.idle":"2023-03-19T22:17:33.739841Z","shell.execute_reply.started":"2023-03-19T22:17:33.731515Z","shell.execute_reply":"2023-03-19T22:17:33.738360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training parameters\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    label_names=label_list,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=1,\n    evaluation_strategy=\"steps\", # should enable do_eval\n    num_train_epochs=1.0,\n    learning_rate=1e-4,\n    fp16=torch.cuda.is_available(), # whether to use fp16 16-bit (mixed) precision training instead of 32-bit training\n    save_steps=100,\n    eval_steps=10,\n    logging_steps=50,\n    report_to=\"wandb\",\n#     report_to=[],\n    half_precision_backend=\"auto\", # shoud be 'cuda_amp' half precision backend\n    gradient_checkpointing=True, # use gradient checkpointing to save memory at the expense of slower backward pass\n)","metadata":{"id":"OeMVreG3-6qs","execution":{"iopub.status.busy":"2023-03-19T22:17:33.741412Z","iopub.execute_input":"2023-03-19T22:17:33.742458Z","iopub.status.idle":"2023-03-19T22:17:33.830012Z","shell.execute_reply.started":"2023-03-19T22:17:33.742421Z","shell.execute_reply":"2023-03-19T22:17:33.828990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“š data\n- ~torchaudio implemented a `dataset` to load IEMOCAP. Later in the script, we train the model with a `Trainer` from hugginface, therefore we prefer translating the pytorch dataset into a `transformers.Dataset` for convenience and compatibility.~\n- the Trainer class expects an argument `train_dataset` to be of type torch.utils.data.Dataset (see [documentation](https://huggingface.co/docs/transformers/main_classes/trainer)) --> we use a torch dataset instead of a Hugginface dataset","metadata":{"id":"w9lfgKcGHlQF"}},{"cell_type":"code","source":"# https://pytorch.org/audio/master/generated/torchaudio.datasets.IEMOCAP.html\nfrom torchaudio.datasets import IEMOCAP\n\nfrom transformers import WhisperProcessor\n\nimport torch\nfrom torch.utils.data import random_split, Dataset, DataLoader, SubsetRandomSampler","metadata":{"id":"63mPgt-MR8p0","execution":{"iopub.status.busy":"2023-03-19T22:17:33.833397Z","iopub.execute_input":"2023-03-19T22:17:33.834114Z","iopub.status.idle":"2023-03-19T22:17:34.430528Z","shell.execute_reply.started":"2023-03-19T22:17:33.834075Z","shell.execute_reply":"2023-03-19T22:17:34.429293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = WhisperProcessor.from_pretrained(model_name_or_path)\ntarget_sampling_rate = processor.feature_extractor.sampling_rate","metadata":{"id":"3h-qBGonTgmf","outputId":"839c58c8-5374-4c0e-d1b6-5e33971ba357","execution":{"iopub.status.busy":"2023-03-19T22:17:34.432302Z","iopub.execute_input":"2023-03-19T22:17:34.432713Z","iopub.status.idle":"2023-03-19T22:17:40.558254Z","shell.execute_reply.started":"2023-03-19T22:17:34.432668Z","shell.execute_reply":"2023-03-19T22:17:40.557269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomIEMOCAP(Dataset):\n  def __init__(self, data, processor):\n    self.data = data\n    self.processor = processor\n\n  def __getitem__(self, index):\n    wav, _, _, label, _ = self.data[index]\n    inputs = self.processor(wav.squeeze(), sampling_rate=target_sampling_rate)\n    inputs[\"labels\"] = label2id[label]\n\n    return inputs\n\n  def __len__(self):\n    return len(self.data)","metadata":{"id":"lEkXSO7-wI2t","execution":{"iopub.status.busy":"2023-03-19T22:17:40.560079Z","iopub.execute_input":"2023-03-19T22:17:40.560465Z","iopub.status.idle":"2023-03-19T22:17:40.567753Z","shell.execute_reply.started":"2023-03-19T22:17:40.560426Z","shell.execute_reply":"2023-03-19T22:17:40.566627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iemocap = IEMOCAP(root=DATA_PATH) # in function, path = root / \"IEMOCAP\"\niemocap = torch.utils.data.Subset(iemocap, range(int(DEBUG_SIZE * len(iemocap)))) # DEBUG\n\ndataset = CustomIEMOCAP(data=iemocap, processor=processor)\ntrain_ds, test_ds = random_split(dataset, [1-test_split_size, test_split_size], generator=torch.Generator().manual_seed(42))\n\ndataset[0]","metadata":{"id":"TkLW4D2ZwYCl","outputId":"3ac8f060-678d-4ab6-e16f-a4e5571ac76e","execution":{"iopub.status.busy":"2023-03-19T22:17:40.569582Z","iopub.execute_input":"2023-03-19T22:17:40.570186Z","iopub.status.idle":"2023-03-19T22:17:47.218075Z","shell.execute_reply.started":"2023-03-19T22:17:40.570149Z","shell.execute_reply":"2023-03-19T22:17:47.216632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸšœ model definition","metadata":{"id":"_DZWd3CGHnnc"}},{"cell_type":"code","source":"from transformers import WhisperModel, PreTrainedModel\nfrom transformers import AutoConfig\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss","metadata":{"id":"OAJrSE-vR4OO","execution":{"iopub.status.busy":"2023-03-19T22:17:47.219797Z","iopub.execute_input":"2023-03-19T22:17:47.220177Z","iopub.status.idle":"2023-03-19T22:17:47.392296Z","shell.execute_reply.started":"2023-03-19T22:17:47.220141Z","shell.execute_reply":"2023-03-19T22:17:47.391297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model configuration\nconfig = AutoConfig.from_pretrained(\n  model_name_or_path,\n  num_labels=num_labels,\n  label2id=label2id,\n  id2label=id2label,\n)\nsetattr(config, 'pooling_mode', pooling_mode)\nsetattr(config, 'keep_n_encoder_layers', keep_n_encoder_layers)\nconfig","metadata":{"id":"a-JbmAoyT6v0","execution":{"iopub.status.busy":"2023-03-19T22:17:47.393572Z","iopub.execute_input":"2023-03-19T22:17:47.395106Z","iopub.status.idle":"2023-03-19T22:17:47.989304Z","shell.execute_reply.started":"2023-03-19T22:17:47.395066Z","shell.execute_reply":"2023-03-19T22:17:47.988271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union, Tuple\nfrom transformers.file_utils import ModelOutput\n\n@dataclass\nclass SpeechClassifierOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None","metadata":{"id":"54M_I54lQ-QJ","execution":{"iopub.status.busy":"2023-03-19T22:17:47.993568Z","iopub.execute_input":"2023-03-19T22:17:47.994379Z","iopub.status.idle":"2023-03-19T22:17:48.002206Z","shell.execute_reply.started":"2023-03-19T22:17:47.994339Z","shell.execute_reply":"2023-03-19T22:17:48.001194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import WhisperPreTrainedModel\n\nclass WhisperClassificationHead(nn.Module):\n  \"\"\"Head for whisper classification task.\"\"\"\n\n  def __init__(self, config):\n    super().__init__()\n    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n    self.dropout = nn.Dropout(config.dropout)\n    self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n  def forward(self, features, **kwargs):\n    x = features\n    x = self.dropout(x)\n    x = self.dense(x)\n    x = torch.tanh(x)\n    x = self.dropout(x)\n    x = self.out_proj(x)\n    return x\n\n\nclass WhisperForSpeechClassification(WhisperPreTrainedModel):\n  def __init__(self, config):\n    super().__init__(config)\n    self.num_labels = config.num_labels\n    self.pooling_mode = config.pooling_mode\n    self.config = config\n\n    self.encoder = WhisperModel(config).encoder\n\n    # only keep first n encoding layers\n    self.encoder.layers = self.encoder.layers[:config.keep_n_encoder_layers]\n    self.classifier = WhisperClassificationHead(config)\n\n    self.init_weights()\n\n  def freeze_encoder(self):\n    self.encoder._freeze_parameters()\n\n  def merged_strategy(\n      \n      self,\n      hidden_states,\n      mode=\"mean\"\n  ):\n    if mode == \"mean\":\n        outputs = torch.mean(hidden_states, dim=1)\n    elif mode == \"sum\":\n        outputs = torch.sum(hidden_states, dim=1)\n    elif mode == \"max\":\n        outputs = torch.max(hidden_states, dim=1)[0]\n    else:\n        raise Exception(\n            \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n\n    return outputs\n\n  def forward(\n      self,\n      input_features,\n      attention_mask=None,\n      output_attentions=None,\n      output_hidden_states=None,\n      return_dict=None,\n      labels=None,\n  ):\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n    outputs = self.encoder(\n        input_features,\n        attention_mask=attention_mask,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    hidden_states = outputs[0]\n    hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n    logits = self.classifier(hidden_states)\n\n    loss = None\n    if labels is not None:\n      loss_fct = CrossEntropyLoss()\n      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n    if not return_dict:\n      output = (logits,) + outputs[2:]\n      return ((loss,) + output) if loss is not None else output\n\n    return SpeechClassifierOutput(\n      loss=loss,\n      logits=logits,\n      hidden_states=outputs.hidden_states,\n      attentions=outputs.attentions,\n    )","metadata":{"id":"P1HzzCqPAuSH","execution":{"iopub.status.busy":"2023-03-19T22:18:52.732523Z","iopub.execute_input":"2023-03-19T22:18:52.733044Z","iopub.status.idle":"2023-03-19T22:18:52.750348Z","shell.execute_reply.started":"2023-03-19T22:18:52.732971Z","shell.execute_reply":"2023-03-19T22:18:52.749366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = WhisperForSpeechClassification.from_pretrained(\n    model_name_or_path,\n    config=config,\n)","metadata":{"id":"C7aPeO-fGBt-","outputId":"656e6891-b37c-4aca-d097-1578eac8ac6a","execution":{"iopub.status.busy":"2023-03-19T22:18:52.752404Z","iopub.execute_input":"2023-03-19T22:18:52.752856Z","iopub.status.idle":"2023-03-19T22:18:53.682213Z","shell.execute_reply.started":"2023-03-19T22:18:52.752820Z","shell.execute_reply":"2023-03-19T22:18:53.681253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.freeze_encoder()\nmodel","metadata":{"id":"qlYFwx5XTCjE","outputId":"b0fc5a76-b5d5-433f-b2e9-63b6c6a2a9f8","execution":{"iopub.status.busy":"2023-03-19T22:18:53.684048Z","iopub.execute_input":"2023-03-19T22:18:53.684805Z","iopub.status.idle":"2023-03-19T22:18:53.692830Z","shell.execute_reply.started":"2023-03-19T22:18:53.684766Z","shell.execute_reply":"2023-03-19T22:18:53.691577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸƒâ€â™€ï¸ training routine","metadata":{"id":"hpjqc0qlH5u0"}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Dict, Union, Tuple, Optional\nfrom packaging import version\nimport numpy as np\n\nimport torch\nfrom torch import nn\n\nfrom transformers import Trainer, is_apex_available, WhisperProcessor, EvalPrediction\n\nif is_apex_available():\n    from apex import amp \n    # Apex is a PyTorch add-on package from NVIDIA with capabilities for automatic mixed precision (AMP) and distributed training.\n    # https://www.ibm.com/docs/en/wmlce/1.6.1?topic=frameworks-getting-started-apex\n\nif version.parse(torch.__version__) >= version.parse(\"1.6\"):\n  _is_native_amp_available = True\n  from torch.cuda.amp import autocast","metadata":{"id":"3frD5uDaR872","execution":{"iopub.status.busy":"2023-03-19T22:18:53.696354Z","iopub.execute_input":"2023-03-19T22:18:53.696715Z","iopub.status.idle":"2023-03-19T22:18:53.705899Z","shell.execute_reply.started":"2023-03-19T22:18:53.696611Z","shell.execute_reply":"2023-03-19T22:18:53.704912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorCTCWithPadding:\n  processor: WhisperProcessor\n  padding: Union[bool, str] = True\n  max_length: Optional[int] = None\n  max_length_labels: Optional[int] = None\n  pad_to_multiple_of: Optional[int] = None\n  pad_to_multiple_of_labels: Optional[int] = None\n\n  def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n    input_features = [{\"input_features\": feature[\"input_features\"][0]} for feature in features]\n    label_features = [feature[\"labels\"] for feature in features]\n\n    d_type = torch.long if isinstance(label_features[0], int) else torch.float\n\n    batch = self.processor.feature_extractor.pad(\n      input_features,\n      padding=self.padding,\n      max_length=self.max_length,\n      pad_to_multiple_of=self.pad_to_multiple_of,\n      return_tensors=\"pt\",\n    )\n\n    batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n    return batch","metadata":{"id":"goTHilxaR3R5","execution":{"iopub.status.busy":"2023-03-19T22:18:53.707688Z","iopub.execute_input":"2023-03-19T22:18:53.708489Z","iopub.status.idle":"2023-03-19T22:18:53.717806Z","shell.execute_reply.started":"2023-03-19T22:18:53.708451Z","shell.execute_reply":"2023-03-19T22:18:53.717068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    preds = np.argmax(preds, axis=1)\n    # (preds == p.label_ids).astype(np.float32).mean().item()\n    return {k: metric(p.label_ids, preds) for k, metric in metrics.items()}","metadata":{"id":"mqZbCzVJSmMP","execution":{"iopub.status.busy":"2023-03-19T22:18:53.719307Z","iopub.execute_input":"2023-03-19T22:18:53.720105Z","iopub.status.idle":"2023-03-19T22:18:53.733351Z","shell.execute_reply.started":"2023-03-19T22:18:53.720068Z","shell.execute_reply":"2023-03-19T22:18:53.732329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CTCTrainer(Trainer):\n  def training_step(self, model, inputs) -> torch.Tensor:\n    model.train()\n    inputs = self._prepare_inputs(inputs)\n\n    with autocast():\n      # loss = self.compute_loss(model, inputs)\n      loss = model(**inputs).get(\"loss\")\n\n    if self.args.gradient_accumulation_steps > 1:\n      loss = loss / self.args.gradient_accumulation_steps\n\n    self.scaler.scale(loss).backward()\n\n    return loss.detach()\n  \n  def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys) -> torch.Tensor: \n    model.eval()\n    inputs = self._prepare_inputs(inputs)\n\n    labels = inputs.get(\"labels\")\n\n    with autocast():\n      outputs = model(**inputs)\n      logits = outputs.get(\"logits\")\n      loss = outputs.get(\"loss\")\n\n    if self.args.gradient_accumulation_steps > 1:\n      loss = loss / self.args.gradient_accumulation_steps\n\n    self.scaler.scale(loss).backward()\n\n    with torch.no_grad():\n      torch.cuda.empty_cache()\n\n    if prediction_loss_only:\n      return loss.detach()\n    return (loss.detach(), logits.detach(), labels.detach())","metadata":{"id":"5NxWSqMKGkRr","execution":{"iopub.status.busy":"2023-03-19T22:18:53.734869Z","iopub.execute_input":"2023-03-19T22:18:53.735276Z","iopub.status.idle":"2023-03-19T22:18:53.745636Z","shell.execute_reply.started":"2023-03-19T22:18:53.735241Z","shell.execute_reply":"2023-03-19T22:18:53.744477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","metadata":{"id":"eH9EZ7o3SWip","execution":{"iopub.status.busy":"2023-03-19T22:18:53.747339Z","iopub.execute_input":"2023-03-19T22:18:53.747744Z","iopub.status.idle":"2023-03-19T22:18:53.757416Z","shell.execute_reply.started":"2023-03-19T22:18:53.747649Z","shell.execute_reply":"2023-03-19T22:18:53.756448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\ntrainer = CTCTrainer(\n  model=model,\n  data_collator=data_collator,\n  args=training_args,\n  compute_metrics=compute_metrics,\n  train_dataset=train_ds,\n  eval_dataset=test_ds,\n  tokenizer=processor.feature_extractor,\n)","metadata":{"id":"d1lLo-_vG8ih","execution":{"iopub.status.busy":"2023-03-19T22:18:53.759057Z","iopub.execute_input":"2023-03-19T22:18:53.759404Z","iopub.status.idle":"2023-03-19T22:18:53.781955Z","shell.execute_reply.started":"2023-03-19T22:18:53.759370Z","shell.execute_reply":"2023-03-19T22:18:53.781054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ§ª experiments","metadata":{"id":"QKJKwY4_IFCx"}},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"D9piW_jCHBav","outputId":"8c360a61-bf6f-4ebf-c65a-c4303a6abaeb","execution":{"iopub.status.busy":"2023-03-19T22:18:53.785044Z","iopub.execute_input":"2023-03-19T22:18:53.785599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.finish()","metadata":{"id":"6y9pes7RLINI","trusted":true},"execution_count":null,"outputs":[]}]}